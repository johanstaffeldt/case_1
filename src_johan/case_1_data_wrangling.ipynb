{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 - Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. **Importing Libraries**\n",
    "\n",
    "2. **Loading Data**\n",
    "\n",
    "3. **Data Wrangling**\n",
    "- Splitting the data into train (80%) and test set (20%)\n",
    "- Continuous Data: Standardizing\n",
    "- Continuous Data: KNN Imputation - Handling missing values by KNNImputer\n",
    "- Categorical Data: Mode Imputation - Handling missing values by SimpleImputer\n",
    "- Categorical Data: One-hot encoding\n",
    "- Saving wrangled data to new .csv file\n",
    "- Summary of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Imputers\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Standardization scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 101)\n",
      "(1000, 100)\n",
      "(100, 101)\n",
      "(1000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Path to the data files\n",
    "data_path_1 = '../data/case1Data.csv'\n",
    "data_path_2 = '../data/case1Data_Xnew.csv'\n",
    "\n",
    "# Load the data into a numpy array\n",
    "data_np = np.loadtxt(data_path_1, delimiter=',', skiprows=1)\n",
    "data_np_new = np.loadtxt(data_path_2, delimiter=',', skiprows=1)\n",
    "\n",
    "# Print the shape of the data in the numpy array\n",
    "print(data_np.shape) # 100 rows and 101 columns (100 features and 1 target)\n",
    "print(data_np_new.shape) # 1000 rows and 100 columns (100 features and no target)\n",
    "\n",
    "# Create a pandas dataframe and use the first row as the column names\n",
    "data_pd = pd.read_csv(data_path_1, sep=',', header=0)\n",
    "data_pd_new = pd.read_csv(data_path_2, sep=',', header=0)\n",
    "\n",
    "# Print the shape of the data in the pandas dataframe\n",
    "print(data_pd.shape)\n",
    "print(data_pd_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train (80%) and test set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (100, 100)\n",
      "y:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into features and target\n",
    "X = data_pd.iloc[:, 1:]\n",
    "y = data_pd.iloc[:, 0]\n",
    "\n",
    "print(\"X: \", X.shape)\n",
    "print(\"y: \", y.shape)\n",
    "\n",
    "# Splitting into train (80%) and test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous data: Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using StandardScaler from scikit-learn to standardize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardizing the numerical features (all columns exept the last five)\n",
    "X_train.iloc[:, :-5] = scaler.fit_transform(X_train.iloc[:, :-5])\n",
    "X_test.iloc[:, :-5] = scaler.transform(X_test.iloc[:, :-5])\n",
    "\n",
    "# Also standardizing the target values\n",
    "y_train = scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten() # reshaping to 1D array\n",
    "y_test = scaler.transform(y_test.values.reshape(-1, 1)).flatten() # reshaping to 1D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Data: KNN Imputation - Handling missing values by KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using KNNImputer from scikit-learn to impute the missing values in the data (for continuous variables) with the mean of the k-nearest neighbors (k=5)\n",
    "\n",
    "# class sklearn.impute.KNNImputer(*, missing_values=nan, n_neighbors=5, weights='uniform', metric='nan_euclidean', copy=True, add_indicator=False, keep_empty_features=False)\n",
    "continuous_imputer = KNNImputer(n_neighbors=5, missing_values=np.nan)\n",
    "\n",
    "# Fitting the imputer on the training data and transforming the training and test data\n",
    "X_train.iloc[:, :-5] = pd.DataFrame(continuous_imputer.fit_transform(X_train.iloc[:, :-5]))\n",
    "X_test.iloc[:, :-5] = pd.DataFrame(continuous_imputer.transform(X_test.iloc[:, :-5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data: Mode Imputation - Handling missing values by SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode Imputation: Using SimpleImputer from scikit-learn to impute the missing values in the data (for categorical variables) with the most frequent value\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fitting the imputer on the training data and transforming the training and test data\n",
    "X_train.iloc[:, -5:] = categorical_imputer.fit_transform(X_train.iloc[:, -5:])\n",
    "X_test.iloc[:, -5:] = categorical_imputer.transform(X_test.iloc[:, -5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data: One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in X_train and X_test should be 100 + 5*number of unique values in the last 5 columns: 116\n",
      "X_train:  (80, 116)\n",
      "X_test:  (20, 116)\n",
      "If the number of columns is >116, one-hot encoding was performed more than once and should be fixed by re-running the code from the beginning.\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the categorical variables using get_dummies from pandas library (for the last five columns)\n",
    "X_train = pd.get_dummies(X_train, columns=X_train.columns[-5:])\n",
    "X_test = pd.get_dummies(X_test, columns=X_test.columns[-5:])\n",
    "\n",
    "# Printing the shape of the data to check if the one-hot encoding worked and only performed once\n",
    "print(\"Number of columns in X_train and X_test should be 100 + 5*number of unique values in the last 5 columns: 116\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"If the number of columns is >116, one-hot encoding was performed more than once and should be fixed by re-running the code from the beginning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving wrangled data to new .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data into numpy arrays\n",
    "X_train = np.asarray(X_train, dtype=np.float64)\n",
    "X_test = np.asarray(X_test, dtype=np.float64)\n",
    "y_train = np.asarray(y_train, dtype=np.float64)\n",
    "y_test = np.asarray(y_test, dtype=np.float64)\n",
    "\n",
    "# Saving the preprocessed data to csv files\n",
    "np.savetxt('../data/case1Data_Xtrain.csv', X_train, delimiter=',')\n",
    "np.savetxt('../data/case1Data_Xtest.csv', X_test, delimiter=',')\n",
    "np.savetxt('../data/case1Data_ytrain.csv', y_train, delimiter=',')\n",
    "np.savetxt('../data/case1Data_ytest.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (80, 116)\n",
      "X_test:  (20, 116)\n",
      "y_train:  (80,)\n",
      "y_test:  (20,)\n",
      "n_train:  80\n",
      "n_test:  20\n",
      "p:  116\n",
      "Number of missing values in X_train:  0\n",
      "Number of missing values in X_test:  0\n",
      "Number of missing values in y_train:  0\n",
      "Number of missing values in y_test:  0\n"
     ]
    }
   ],
   "source": [
    "# Printing the shape of the data\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)\n",
    "\n",
    "# Size of the training and test data\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "p = X_train.shape[1]\n",
    "\n",
    "# Printing the size of the training and test data\n",
    "print(\"n_train: \", n_train) # number of training samples\n",
    "print(\"n_test: \", n_test) # number of test samples\n",
    "print(\"p: \", p) # number of features/variables/columns/parameters\n",
    "\n",
    "# Checking for missing values in the wrangled data\n",
    "missing_values_X_train = np.isnan(X_train)\n",
    "print(\"Number of missing values in X_train: \", np.sum(missing_values_X_train))\n",
    "missing_values_X_test = np.isnan(X_test)\n",
    "print(\"Number of missing values in X_test: \", np.sum(missing_values_X_test))\n",
    "missing_values_y_train = np.isnan(y_train)\n",
    "print(\"Number of missing values in y_train: \", np.sum(missing_values_y_train))\n",
    "missing_values_y_test = np.isnan(y_test)\n",
    "print(\"Number of missing values in y_test: \", np.sum(missing_values_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of the features in the training data:  [0.8995607  0.91036137 0.91800268 0.90829383 0.8895423  0.88680058\n",
      " 0.89491855 0.82829919 0.91975013 0.91469048 0.93937106 0.8652754\n",
      " 0.85952083 0.88880764 0.90906405 0.91796272 0.86431866 0.91924688\n",
      " 0.92685719 0.8882272  0.88584175 0.88421285 0.92238152 0.87779346\n",
      " 0.94719134 0.92035477 0.88257315 0.92899084 0.8945435  0.92387824\n",
      " 0.91465276 0.8824306  0.96988645 0.90546    0.92140432 0.92122569\n",
      " 0.89846144 0.95296966 0.88644565 0.8558488  0.92020291 0.93305861\n",
      " 0.93423706 0.91524159 0.92911629 0.91830368 0.91340884 0.96206551\n",
      " 0.87977695 0.85946268 0.84717089 0.91141559 0.96468983 0.86398192\n",
      " 0.86828733 0.89186396 0.8753154  0.84689926 0.89752627 0.82636267\n",
      " 0.94060279 0.89127463 0.94269088 0.92686018 0.86038169 0.89301074\n",
      " 0.89394395 0.91614871 0.88429546 0.92308286 0.8831762  0.89655477\n",
      " 0.87375987 0.87581971 0.86562372 0.95387655 0.92084129 0.89047188\n",
      " 0.89788237 0.93193157 0.92083104 0.88406959 0.89728524 0.87668852\n",
      " 0.76871507 0.91194504 0.90566421 0.89021744 0.92920302 0.88704377\n",
      " 0.96200997 0.83654992 0.89920889 0.92640168 0.9145472  0.1275\n",
      " 0.11859375 0.109375   0.24859375 0.109375   0.1275     0.24234375\n",
      " 0.13609375 0.13609375 0.11859375 0.249375   0.144375   0.144375\n",
      " 0.109375   0.249375   0.109375  ]\n",
      "Covariance between the features and the target in the training data:  [[ 0.91094754  0.34461735  0.38879107 ...  0.17769522 -0.08999062\n",
      "   0.13334713]\n",
      " [ 0.34461735  0.92188493  0.45146135 ...  0.12795237 -0.06995562\n",
      "   0.04008476]\n",
      " [ 0.38879107  0.45146135  0.92962297 ...  0.09129606 -0.02750138\n",
      "   0.24620463]\n",
      " ...\n",
      " [ 0.17769522  0.12795237  0.09129606 ...  0.25253165 -0.06012658\n",
      "   0.09587148]\n",
      " [-0.08999062 -0.06995562 -0.02750138 ... -0.06012658  0.11075949\n",
      "  -0.04615582]\n",
      " [ 0.13334713  0.04008476  0.24620463 ...  0.09587148 -0.04615582\n",
      "   1.01265823]]\n",
      "Variance of the features in the training data:  [0.8995607  0.91036137 0.91800268 0.90829383 0.8895423  0.88680058\n",
      " 0.89491855 0.82829919 0.91975013 0.91469048 0.93937106 0.8652754\n",
      " 0.85952083 0.88880764 0.90906405 0.91796272 0.86431866 0.91924688\n",
      " 0.92685719 0.8882272  0.88584175 0.88421285 0.92238152 0.87779346\n",
      " 0.94719134 0.92035477 0.88257315 0.92899084 0.8945435  0.92387824\n",
      " 0.91465276 0.8824306  0.96988645 0.90546    0.92140432 0.92122569\n",
      " 0.89846144 0.95296966 0.88644565 0.8558488  0.92020291 0.93305861\n",
      " 0.93423706 0.91524159 0.92911629 0.91830368 0.91340884 0.96206551\n",
      " 0.87977695 0.85946268 0.84717089 0.91141559 0.96468983 0.86398192\n",
      " 0.86828733 0.89186396 0.8753154  0.84689926 0.89752627 0.82636267\n",
      " 0.94060279 0.89127463 0.94269088 0.92686018 0.86038169 0.89301074\n",
      " 0.89394395 0.91614871 0.88429546 0.92308286 0.8831762  0.89655477\n",
      " 0.87375987 0.87581971 0.86562372 0.95387655 0.92084129 0.89047188\n",
      " 0.89788237 0.93193157 0.92083104 0.88406959 0.89728524 0.87668852\n",
      " 0.76871507 0.91194504 0.90566421 0.89021744 0.92920302 0.88704377\n",
      " 0.96200997 0.83654992 0.89920889 0.92640168 0.9145472  0.1275\n",
      " 0.11859375 0.109375   0.24859375 0.109375   0.1275     0.24234375\n",
      " 0.13609375 0.13609375 0.11859375 0.249375   0.144375   0.144375\n",
      " 0.109375   0.249375   0.109375  ]\n"
     ]
    }
   ],
   "source": [
    "## Calculating the variance of the features in the training data\n",
    "#var_X_train = np.var(X_train, axis=0)\n",
    "#print(\"Variance of the features in the training data: \", var_X_train)\n",
    "#\n",
    "## Calculating the covariance between the features and the target in the training data\n",
    "#cov_X_train_y_train = np.cov(X_train.T, y_train)\n",
    "#print(\"Covariance between the features and the target in the training data: \", cov_X_train_y_train)\n",
    "#\n",
    "## Removing all features with variance below 0.1\n",
    "#X_train = X_train[:, var_X_train > 0.2]\n",
    "#\n",
    "## Calculating the variance of the features in the training data\n",
    "#var_X_train = np.var(X_train, axis=0)\n",
    "#print(\"Variance of the features in the training data: \", var_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices: [  6   8   9  10  12  27  28  29  31  34  35  40  42  44  48  50  53  54\n",
      "  56  58  61  63  67  72  74  75  77  81  82  85 112]\n",
      "X_train_selected:  (80, 31)\n",
      "X_test_selected:  (20, 31)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lasso = LassoCV(cv=3, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Select important features (nonzero coefficients)\n",
    "selector = SelectFromModel(lasso, prefit=True)\n",
    "selected_features_mask = selector.get_support()\n",
    "selected_features = np.where(selected_features_mask)[0]  # Get indices of selected features\n",
    "\n",
    "print(\"Selected Feature Indices:\", selected_features)\n",
    "\n",
    "X_selected = X_train[:, selected_features]\n",
    "\n",
    "# Modify the train and test data to only include the selected features\n",
    "X_train_selected = X_train[:, selected_features]\n",
    "X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "print(\"X_train_selected: \", X_train_selected.shape)\n",
    "print(\"X_test_selected: \", X_test_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices: [31, 61, 35, 53, 67, 36, 81, 48, 9, 38, 77, 74, 104]\n",
      "Best BIC Score: -185.95335366992532\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def compute_bic(X, y, model):\n",
    "    \"\"\"Compute Bayesian Information Criterion (BIC) for a given model.\"\"\"\n",
    "    n, k = X.shape  # n = samples, k = features\n",
    "    model.fit(X, y)\n",
    "    residuals = y - model.predict(X)\n",
    "    rss = np.sum(residuals**2)  # Residual sum of squares\n",
    "    sigma2 = rss / n  # Estimated variance\n",
    "    bic = n * np.log(sigma2) + k * np.log(n)  # BIC formula\n",
    "    return bic\n",
    "\n",
    "def stepwise_bic_selection(X, y):\n",
    "    \"\"\"Greedy forward selection based on BIC.\"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    selected_features = []  # Start with an empty set\n",
    "    best_bic = np.inf\n",
    "    model = LinearRegression()\n",
    "\n",
    "    while True:\n",
    "        bic_scores = []\n",
    "        candidates = [i for i in range(n_features) if i not in selected_features]\n",
    "\n",
    "        # Try adding each remaining feature\n",
    "        for feature in candidates:\n",
    "            current_features = selected_features + [feature]\n",
    "            bic = compute_bic(X[:, current_features], y, model)\n",
    "            bic_scores.append((feature, bic))\n",
    "\n",
    "        # Select the feature that minimizes BIC\n",
    "        bic_scores.sort(key=lambda x: x[1])\n",
    "        best_new_feature, new_bic = bic_scores[0]\n",
    "\n",
    "        # Stop if BIC does not improve\n",
    "        if new_bic >= best_bic:\n",
    "            break\n",
    "\n",
    "        # Otherwise, update the best BIC and selected features\n",
    "        best_bic = new_bic\n",
    "        selected_features.append(best_new_feature)\n",
    "\n",
    "    return selected_features, best_bic\n",
    "\n",
    "# Run BIC feature selection\n",
    "best_features, best_bic = stepwise_bic_selection(X_train, y_train)\n",
    "\n",
    "print(\"Selected Feature Indices:\", best_features)\n",
    "print(\"Best BIC Score:\", best_bic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
