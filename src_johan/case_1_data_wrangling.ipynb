{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 - Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. **Importing Libraries**\n",
    "\n",
    "2. **Loading Data**\n",
    "\n",
    "3. **Data Wrangling**\n",
    "- Splitting the data into train (80%) and test set (20%)\n",
    "- Continuous Data: Standardizing\n",
    "- Continuous Data: KNN Imputation - Handling missing values by KNNImputer\n",
    "- Categorical Data: Mode Imputation - Handling missing values by SimpleImputer\n",
    "- Categorical Data: One-hot encoding\n",
    "- Saving wrangled data to new .csv file\n",
    "- Summary of the data\n",
    "\n",
    "4. **Data Wrangling for Nested CV (Without Splitting)**\n",
    "\n",
    "5. **Wrangling case1Data_Xnew.csv**\n",
    "\n",
    "6. **Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Imputers\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Standardization scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 101)\n",
      "(1000, 100)\n",
      "(100, 101)\n",
      "(1000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Path to the data files\n",
    "data_path_1 = '../data/case1Data.csv'\n",
    "data_path_2 = '../data/case1Data_Xnew.csv'\n",
    "\n",
    "# Load the data into a numpy array\n",
    "data_np = np.loadtxt(data_path_1, delimiter=',', skiprows=1)\n",
    "data_np_new = np.loadtxt(data_path_2, delimiter=',', skiprows=1)\n",
    "\n",
    "# Print the shape of the data in the numpy array\n",
    "print(data_np.shape) # 100 rows and 101 columns (100 features and 1 target)\n",
    "print(data_np_new.shape) # 1000 rows and 100 columns (100 features and no target)\n",
    "\n",
    "# Create a pandas dataframe and use the first row as the column names\n",
    "data_pd = pd.read_csv(data_path_1, sep=',', header=0)\n",
    "data_pd_new = pd.read_csv(data_path_2, sep=',', header=0)\n",
    "\n",
    "# Print the shape of the data in the pandas dataframe\n",
    "print(data_pd.shape)\n",
    "print(data_pd_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train (80%) and test set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (100, 100)\n",
      "y:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into features and target\n",
    "X = data_pd.iloc[:, 1:]\n",
    "y = data_pd.iloc[:, 0]\n",
    "\n",
    "print(\"X: \", X.shape)\n",
    "print(\"y: \", y.shape)\n",
    "\n",
    "# Splitting into train (80%) and test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous data: Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using StandardScaler from scikit-learn to standardize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardizing the numerical features (all columns exept the last five)\n",
    "X_train.iloc[:, :-5] = scaler.fit_transform(X_train.iloc[:, :-5])\n",
    "X_test.iloc[:, :-5] = scaler.transform(X_test.iloc[:, :-5])\n",
    "\n",
    "# Also standardizing the target values\n",
    "#y_train = scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten() # reshaping to 1D array\n",
    "#y_test = scaler.transform(y_test.values.reshape(-1, 1)).flatten() # reshaping to 1D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Data: KNN Imputation - Handling missing values by KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using KNNImputer from scikit-learn to impute the missing values in the data (for continuous variables) with the mean of the k-nearest neighbors (k=5)\n",
    "\n",
    "# class sklearn.impute.KNNImputer(*, missing_values=nan, n_neighbors=5, weights='uniform', metric='nan_euclidean', copy=True, add_indicator=False, keep_empty_features=False)\n",
    "continuous_imputer = KNNImputer(n_neighbors=5, missing_values=np.nan)\n",
    "\n",
    "# Fitting the imputer on the training data and transforming the training and test data\n",
    "X_train.iloc[:, :-5] = pd.DataFrame(continuous_imputer.fit_transform(X_train.iloc[:, :-5]))\n",
    "X_test.iloc[:, :-5] = pd.DataFrame(continuous_imputer.transform(X_test.iloc[:, :-5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data: Mode Imputation - Handling missing values by SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode Imputation: Using SimpleImputer from scikit-learn to impute the missing values in the data (for categorical variables) with the most frequent value\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fitting the imputer on the training data and transforming the training and test data\n",
    "X_train.iloc[:, -5:] = categorical_imputer.fit_transform(X_train.iloc[:, -5:])\n",
    "X_test.iloc[:, -5:] = categorical_imputer.transform(X_test.iloc[:, -5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data: One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in X_train and X_test should be 100 + 5*number of unique values in the last 5 columns: 116\n",
      "X_train:  (80, 116)\n",
      "X_test:  (20, 116)\n",
      "If the number of columns is >116, one-hot encoding was performed more than once and should be fixed by re-running the code from the beginning.\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the categorical variables using get_dummies from pandas library (for the last five columns)\n",
    "X_train = pd.get_dummies(X_train, columns=X_train.columns[-5:])\n",
    "X_test = pd.get_dummies(X_test, columns=X_test.columns[-5:])\n",
    "\n",
    "# Printing the shape of the data to check if the one-hot encoding worked and only performed once\n",
    "print(\"Number of columns in X_train and X_test should be 100 + 5*number of unique values in the last 5 columns: 116\")\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"If the number of columns is >116, one-hot encoding was performed more than once and should be fixed by re-running the code from the beginning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving wrangled data to new .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data into numpy arrays\n",
    "X_train = np.asarray(X_train, dtype=np.float64)\n",
    "X_test = np.asarray(X_test, dtype=np.float64)\n",
    "y_train = np.asarray(y_train, dtype=np.float64)\n",
    "y_test = np.asarray(y_test, dtype=np.float64)\n",
    "\n",
    "# Saving the preprocessed data to csv files\n",
    "np.savetxt('../data/case1Data_Xtrain.csv', X_train, delimiter=',')\n",
    "np.savetxt('../data/case1Data_Xtest.csv', X_test, delimiter=',')\n",
    "np.savetxt('../data/case1Data_ytrain.csv', y_train, delimiter=',')\n",
    "np.savetxt('../data/case1Data_ytest.csv', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (80, 116)\n",
      "X_test:  (20, 116)\n",
      "y_train:  (80,)\n",
      "y_test:  (20,)\n",
      "n_train:  80\n",
      "n_test:  20\n",
      "p:  116\n",
      "Number of missing values in X_train:  0\n",
      "Number of missing values in X_test:  0\n",
      "Number of missing values in y_train:  0\n",
      "Number of missing values in y_test:  0\n"
     ]
    }
   ],
   "source": [
    "# Printing the shape of the data\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)\n",
    "\n",
    "# Size of the training and test data\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "p = X_train.shape[1]\n",
    "\n",
    "# Printing the size of the training and test data\n",
    "print(\"n_train: \", n_train) # number of training samples\n",
    "print(\"n_test: \", n_test) # number of test samples\n",
    "print(\"p: \", p) # number of features/variables/columns/parameters\n",
    "\n",
    "# Checking for missing values in the wrangled data\n",
    "missing_values_X_train = np.isnan(X_train)\n",
    "print(\"Number of missing values in X_train: \", np.sum(missing_values_X_train))\n",
    "missing_values_X_test = np.isnan(X_test)\n",
    "print(\"Number of missing values in X_test: \", np.sum(missing_values_X_test))\n",
    "missing_values_y_train = np.isnan(y_train)\n",
    "print(\"Number of missing values in y_train: \", np.sum(missing_values_y_train))\n",
    "missing_values_y_test = np.isnan(y_test)\n",
    "print(\"Number of missing values in y_test: \", np.sum(missing_values_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Wrangling for Nested CV (Without Splitting, Standardizing, and Handling Missing Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (100, 100)\n",
      "y:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data into a numpy array\n",
    "data = np.loadtxt('../data/case1Data.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = data[:, 1:] # All columns except the first one\n",
    "y = data[:, 0] # First column\n",
    "print(\"X: \", X.shape)\n",
    "print(\"y: \", y.shape)\n",
    "\n",
    "## Using StandardScaler from scikit-learn to standardize the data\n",
    "#scaler = StandardScaler()\n",
    "#\n",
    "## Converting X into a pandas dataframe\n",
    "#X = pd.DataFrame(X)\n",
    "#\n",
    "## Standardizing the numerical features (all columns exept the last five)\n",
    "#X.iloc[:, :-5] = scaler.fit_transform(X.iloc[:, :-5])\n",
    "#\n",
    "## Using KNNImputer from scikit-learn to impute the missing values in the data (for continuous variables) with the mean of the k-nearest neighbors (k=5)\n",
    "#continuous_imputer = KNNImputer(n_neighbors=5, missing_values=np.nan)\n",
    "#X.iloc[:, :-5] = pd.DataFrame(continuous_imputer.fit_transform(X.iloc[:, :-5]))\n",
    "#\n",
    "## Mode Imputation: Using SimpleImputer from scikit-learn to impute the missing values in the data (for categorical variables) with the most frequent value\n",
    "#categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "#X.iloc[:, -5:] = categorical_imputer.fit_transform(X.iloc[:, -5:])\n",
    "#\n",
    "## One-hot encoding the categorical variables using get_dummies from pandas library (for the last five columns)\n",
    "#X = pd.get_dummies(X, columns=X.columns[-5:])\n",
    "#\n",
    "## Printing the shape of the data to check if the one-hot encoding worked and only performed once\n",
    "#print(\"Number of columns in X should be 100 + 5*number of unique values in the last 5 columns: 116\")\n",
    "#print(\"X: \", X.shape)\n",
    "#print(\"If the number of columns is >116, one-hot encoding was performed more than once and should be fixed by re-running the code from the beginning.\")\n",
    "\n",
    "# Converting the data into numpy arrays\n",
    "X = np.asarray(X, dtype=np.float64)\n",
    "\n",
    "# Saving the preprocessed data to a csv file\n",
    "np.savetxt('../data/case1Data_X.csv', X, delimiter=',')\n",
    "np.savetxt('../data/case1Data_y.csv', y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wrangling case1Data_Xnew.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new:  (1000, 100)\n",
      "Number of columns in X_new should be 100 + 5*number of unique values in the last 5 columns: 116\n",
      "X_new:  (1000, 116)\n",
      "If the number of columns is >116, one-hot encoding was performed more than once and should be fixed by re-running the code from the beginning.\n",
      "X_new:  (1000, 116)\n"
     ]
    }
   ],
   "source": [
    "# Loading the new data into a numpy array\n",
    "X_new = pd.DataFrame(np.loadtxt('../data/case1Data_Xnew.csv', delimiter=',', skiprows=1))\n",
    "print(\"X_new: \", X_new.shape)\n",
    "\n",
    "# Using StandardScaler from scikit-learn to standardize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardizing the numerical features (all columns exept the last five)\n",
    "X_new.iloc[:, :-5] = scaler.fit_transform(X_new.iloc[:, :-5])\n",
    "\n",
    "# Using KNNImputer from scikit-learn to impute the missing values in the data (for continuous variables) with the mean of the k-nearest neighbors (k=5)\n",
    "X_new.iloc[:, :-5] = pd.DataFrame(continuous_imputer.fit_transform(X_new.iloc[:, :-5]))\n",
    "\n",
    "# Mode Imputation: Using SimpleImputer from scikit-learn to impute the missing values in the data (for categorical variables) with the most frequent value\n",
    "X_new.iloc[:, -5:] = categorical_imputer.fit_transform(X_new.iloc[:, -5:])\n",
    "\n",
    "# One-hot encoding the categorical variables using get_dummies from pandas library (for the last five columns)\n",
    "X_new = pd.get_dummies(X_new, columns=X_new.columns[-5:])\n",
    "\n",
    "# Printing the shape of the data to check if the one-hot encoding worked and only performed once\n",
    "print(\"Number of columns in X_new should be 100 + 5*number of unique values in the last 5 columns: 116\")\n",
    "print(\"X_new: \", X_new.shape)\n",
    "print(\"If the number of columns is >116, one-hot encoding was performed more than once and should be fixed by re-running the code from the beginning.\")\n",
    "\n",
    "# Converting the data into numpy arrays\n",
    "X_new = np.asarray(X_new, dtype=np.float64)\n",
    "\n",
    "# Saving the preprocessed data to a csv file\n",
    "np.savetxt('../data/case1Data_Xnew_wrangled.csv', X_new, delimiter=',')\n",
    "print(\"X_new: \", X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the variance of the features in the training data\n",
    "#var_X_train = np.var(X_train, axis=0)\n",
    "#print(\"Variance of the features in the training data: \", var_X_train)\n",
    "#\n",
    "## Calculating the covariance between the features and the target in the training data\n",
    "#cov_X_train_y_train = np.cov(X_train.T, y_train)\n",
    "#print(\"Covariance between the features and the target in the training data: \", cov_X_train_y_train)\n",
    "#\n",
    "## Removing all features with variance below 0.1\n",
    "#X_train = X_train[:, var_X_train > 0.2]\n",
    "#\n",
    "## Calculating the variance of the features in the training data\n",
    "#var_X_train = np.var(X_train, axis=0)\n",
    "#print(\"Variance of the features in the training data: \", var_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices: [  6   8   9  10  12  27  28  29  31  34  35  40  42  44  48  50  53  54\n",
      "  56  58  61  63  67  72  74  75  77  81  82  85 112]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lasso = LassoCV(cv=3, random_state=42) # LassoCV automatically performs cross-validation\n",
    "lasso.fit(X_train, y_train) # Fit the model on the training data\n",
    "\n",
    "# Select important features (nonzero coefficients)\n",
    "selector = SelectFromModel(lasso, prefit=True) # Use the Lasso model to select features\n",
    "selected_features_mask = selector.get_support()\n",
    "selected_features = np.where(selected_features_mask)[0]  # Get indices of selected features\n",
    "\n",
    "print(\"Selected Feature Indices:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices: [31, 61, 35, 53, 67, 36, 81, 48, 9, 38, 77, 74, 104]\n",
      "Best BIC Score: 492.75769252942996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def compute_bic(X, y, model):\n",
    "    \"\"\"Compute Bayesian Information Criterion (BIC) for a given model.\"\"\"\n",
    "    n, k = X.shape  # n = samples, k = features\n",
    "    model.fit(X, y)\n",
    "    residuals = y - model.predict(X)\n",
    "    rss = np.sum(residuals**2)  # Residual sum of squares\n",
    "    sigma2 = rss / n  # Estimated variance\n",
    "    bic = n * np.log(sigma2) + k * np.log(n)  # BIC formula\n",
    "    return bic\n",
    "\n",
    "def stepwise_bic_selection(X, y):\n",
    "    \"\"\"Greedy forward selection based on BIC.\"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    selected_features = []  # Start with an empty set\n",
    "    best_bic = np.inf\n",
    "    model = LinearRegression()\n",
    "\n",
    "    while True:\n",
    "        bic_scores = []\n",
    "        candidates = [i for i in range(n_features) if i not in selected_features]\n",
    "\n",
    "        # Try adding each remaining feature\n",
    "        for feature in candidates:\n",
    "            current_features = selected_features + [feature]\n",
    "            bic = compute_bic(X[:, current_features], y, model)\n",
    "            bic_scores.append((feature, bic))\n",
    "\n",
    "        # Select the feature that minimizes BIC\n",
    "        bic_scores.sort(key=lambda x: x[1])\n",
    "        best_new_feature, new_bic = bic_scores[0]\n",
    "\n",
    "        # Stop if BIC does not improve\n",
    "        if new_bic >= best_bic:\n",
    "            break\n",
    "\n",
    "        # Otherwise, update the best BIC and selected features\n",
    "        best_bic = new_bic\n",
    "        selected_features.append(best_new_feature)\n",
    "\n",
    "    return selected_features, best_bic\n",
    "\n",
    "# Run BIC feature selection\n",
    "best_features, best_bic = stepwise_bic_selection(X_train, y_train)\n",
    "\n",
    "print(\"Selected Feature Indices:\", best_features)\n",
    "print(\"Best BIC Score:\", best_bic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
