{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 - Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. **Importing Libraries**\n",
    "\n",
    "2. **Loading Data**\n",
    "\n",
    "3. **Random Forest**\n",
    "\n",
    "4. **AdaBoosting**\n",
    "\n",
    "5. **Ensemble of Elastic Net Predictions + AdaBoost Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data into numpy arrays\n",
    "X_train = np.loadtxt('../data/case1Data_Xtrain.csv', delimiter=',')\n",
    "X_test = np.loadtxt('../data/case1Data_Xtest.csv', delimiter=',')\n",
    "y_train = np.loadtxt('../data/case1Data_ytrain.csv', delimiter=',')\n",
    "y_test = np.loadtxt('../data/case1Data_ytest.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be similar to the summary in case_1_data_wrangling.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (80, 116)\n",
      "X_test:  (20, 116)\n",
      "y_train:  (80,)\n",
      "y_test:  (20,)\n",
      "n_train:  80\n",
      "n_test:  20\n",
      "p:  116\n",
      "Number of missing values in X_train:  0\n",
      "Number of missing values in X_test:  0\n",
      "Number of missing values in y_train:  0\n",
      "Number of missing values in y_test:  0\n"
     ]
    }
   ],
   "source": [
    "# Printing the shape of the data\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)\n",
    "\n",
    "# Size of the training and test data\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "p = X_train.shape[1]\n",
    "\n",
    "# Printing the size of the training and test data\n",
    "print(\"n_train: \", n_train) # number of training samples\n",
    "print(\"n_test: \", n_test) # number of test samples\n",
    "print(\"p: \", p) # number of features/variables/columns/parameters\n",
    "\n",
    "# Checking for missing values in the wrangled data\n",
    "missing_values_X_train = np.isnan(X_train)\n",
    "print(\"Number of missing values in X_train: \", np.sum(missing_values_X_train))\n",
    "missing_values_X_test = np.isnan(X_test)\n",
    "print(\"Number of missing values in X_test: \", np.sum(missing_values_X_test))\n",
    "missing_values_y_train = np.isnan(y_train)\n",
    "print(\"Number of missing values in y_train: \", np.sum(missing_values_y_train))\n",
    "missing_values_y_test = np.isnan(y_test)\n",
    "print(\"Number of missing values in y_test: \", np.sum(missing_values_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELECTING FEATURES WITH BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices: [31, 61, 35, 53, 67, 36, 81, 48, 9, 38, 77, 74, 104]\n",
      "Best BIC Score: 492.75769252942996\n"
     ]
    }
   ],
   "source": [
    "def compute_bic(X, y, model):\n",
    "    \"\"\"Compute Bayesian Information Criterion (BIC) for a given model.\"\"\"\n",
    "    n, k = X.shape  # n = samples, k = features\n",
    "    model.fit(X, y)\n",
    "    residuals = y - model.predict(X)\n",
    "    rss = np.sum(residuals**2)  # Residual sum of squares\n",
    "    sigma2 = rss / n  # Estimated variance\n",
    "    bic = n * np.log(sigma2) + k * np.log(n)  # BIC formula\n",
    "    return bic\n",
    "\n",
    "def stepwise_bic_selection(X, y):\n",
    "    \"\"\"Greedy forward selection based on BIC.\"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    selected_features = []  # Start with an empty set\n",
    "    best_bic = np.inf\n",
    "    model = LinearRegression()\n",
    "\n",
    "    while True:\n",
    "        bic_scores = []\n",
    "        candidates = [i for i in range(n_features) if i not in selected_features]\n",
    "\n",
    "        # Try adding each remaining feature\n",
    "        for feature in candidates:\n",
    "            current_features = selected_features + [feature]\n",
    "            bic = compute_bic(X[:, current_features], y, model)\n",
    "            bic_scores.append((feature, bic))\n",
    "\n",
    "        # Select the feature that minimizes BIC\n",
    "        bic_scores.sort(key=lambda x: x[1])\n",
    "        best_new_feature, new_bic = bic_scores[0]\n",
    "\n",
    "        # Stop if BIC does not improve\n",
    "        if new_bic >= best_bic:\n",
    "            break\n",
    "\n",
    "        # Otherwise, update the best BIC and selected features\n",
    "        best_bic = new_bic\n",
    "        selected_features.append(best_new_feature)\n",
    "\n",
    "    return selected_features, best_bic\n",
    "\n",
    "# Run BIC feature selection\n",
    "best_features, best_bic = stepwise_bic_selection(X_train, y_train)\n",
    "\n",
    "print(\"Selected Feature Indices:\", best_features)\n",
    "print(\"Best BIC Score:\", best_bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELECTING FEATURES WITH LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices: [  6   8   9  10  12  27  28  29  31  34  35  40  42  44  48  50  53  54\n",
      "  56  58  61  63  67  72  74  75  77  81  82  85 112]\n"
     ]
    }
   ],
   "source": [
    "lasso = LassoCV(cv=3, random_state=42) # LassoCV automatically performs cross-validation\n",
    "lasso.fit(X_train, y_train) # Fit the model on the training data\n",
    "\n",
    "# Select important features (nonzero coefficients)\n",
    "selector = SelectFromModel(lasso, prefit=True) # Use the Lasso model to select features\n",
    "selected_features_mask = selector.get_support()\n",
    "selected_features = np.where(selected_features_mask)[0]  # Get indices of selected features\n",
    "\n",
    "print(\"Selected Feature Indices:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for random forest: 66.0736\n"
     ]
    }
   ],
   "source": [
    "# Initializing RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=42)\n",
    "\n",
    "# Performing cross-validation\n",
    "cv_scores = cross_val_score(rf, X_train, y_train, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Training the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_hat = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_hat))\n",
    "print(f'RMSE for random forest: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WITH BIC FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for random forest: 49.7670\n"
     ]
    }
   ],
   "source": [
    "# Initializing RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=42)\n",
    "\n",
    "# Performing cross-validation\n",
    "cv_scores = cross_val_score(rf, X_train[:, best_features], y_train, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Training the model\n",
    "rf.fit(X_train[:, best_features], y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_hat = rf.predict(X_test[:, best_features])\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_hat))\n",
    "print(f'RMSE for random forest: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WITH LASSO FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for random forest: 57.2986\n"
     ]
    }
   ],
   "source": [
    "# Initializing RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=42)\n",
    "\n",
    "# Performing cross-validation\n",
    "cv_scores = cross_val_score(rf, X_train[:, selected_features], y_train, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Training the model\n",
    "rf.fit(X_train[:, selected_features], y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_hat = rf.predict(X_test[:, selected_features])\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_hat))\n",
    "print(f'RMSE for random forest: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AdaBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Test RMSE for AdaBoost: 49.2624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# Define parameter distributions\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'estimator__max_depth': [1, 3, 5, 10, 15],\n",
    "    'learning_rate': np.logspace(-2, 1, 50),\n",
    "}\n",
    "\n",
    "# Create AdaBoostRegressor with DecisionTreeRegressor as base estimator\n",
    "boost = AdaBoostRegressor(estimator=DecisionTreeRegressor())\n",
    "\n",
    "# Use RandomizedSearchCV for efficiency\n",
    "boost_search = RandomizedSearchCV(\n",
    "    boost,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=300,  # Adjust based on computational resources\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "boost_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "best_params = boost_search.best_params_\n",
    "best_score = -boost_search.best_score_  # Converting back to positive MSE\n",
    "\n",
    "# Use the best model to make predictions\n",
    "y_hat = boost_search.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_hat))\n",
    "print(f'Test RMSE for AdaBoost: {rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Test RMSE for AdaBoost: 51.8028\n"
     ]
    }
   ],
   "source": [
    "### AdaBoost only using the continuous features\n",
    "\n",
    "# Define parameter distributions\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'estimator__max_depth': [1, 3, 5, 10, 15],\n",
    "    'learning_rate': np.logspace(-2, 1, 50),\n",
    "}\n",
    "\n",
    "# Create AdaBoostRegressor with DecisionTreeRegressor as base estimator\n",
    "boost = AdaBoostRegressor(estimator=DecisionTreeRegressor())\n",
    "\n",
    "# Use RandomizedSearchCV for efficiency\n",
    "boost_search = RandomizedSearchCV(\n",
    "    boost,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Adjust based on computational resources\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model by only using the continuous features (first 95 columns)\n",
    "boost_search.fit(X_train[:, :95], y_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "best_params = boost_search.best_params_\n",
    "best_score = -boost_search.best_score_  # Converting back to positive MSE\n",
    "\n",
    "# Use the best model to make predictions\n",
    "y_hat = boost_search.predict(X_test[:, :95])\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_hat))\n",
    "print(f'Test RMSE for AdaBoost: {rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WITH BIC FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Test RMSE for AdaBoost: 42.5136\n"
     ]
    }
   ],
   "source": [
    "# Define parameter distributions\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'estimator__max_depth': [1, 3, 5, 10, 15],\n",
    "    'learning_rate': np.logspace(-2, 1, 50),\n",
    "}\n",
    "\n",
    "# Create AdaBoostRegressor with DecisionTreeRegressor as base estimator\n",
    "boost = AdaBoostRegressor(estimator=DecisionTreeRegressor())\n",
    "\n",
    "# Use RandomizedSearchCV for efficiency\n",
    "boost_search = RandomizedSearchCV(\n",
    "    boost,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=300,  # Adjust based on computational resources\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "boost_search.fit(X_train[:, best_features], y_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "best_params = boost_search.best_params_\n",
    "best_score = -boost_search.best_score_  # Converting back to positive MSE\n",
    "\n",
    "# Use the best model to make predictions\n",
    "y_hat = boost_search.predict(X_test[:, best_features])\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_hat))\n",
    "print(f'Test RMSE for AdaBoost: {rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WITH LASSO FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Test RMSE for AdaBoost: 45.7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# Define parameter distributions\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'estimator__max_depth': [1, 3, 5, 10, 15],\n",
    "    'learning_rate': np.logspace(-2, 1, 50),\n",
    "}\n",
    "\n",
    "# Create AdaBoostRegressor with DecisionTreeRegressor as base estimator\n",
    "boost = AdaBoostRegressor(estimator=DecisionTreeRegressor())\n",
    "\n",
    "# Use RandomizedSearchCV for efficiency\n",
    "boost_search = RandomizedSearchCV(\n",
    "    boost,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=300,  # Adjust based on computational resources\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "boost_search.fit(X_train[:, selected_features], y_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "best_params = boost_search.best_params_\n",
    "best_score = -boost_search.best_score_  # Converting back to positive MSE\n",
    "\n",
    "# Use the best model to make predictions\n",
    "y_hat = boost_search.predict(X_test[:, selected_features])\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_hat))\n",
    "print(f'Test RMSE for AdaBoost: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 45.78660673398\n"
     ]
    }
   ],
   "source": [
    "# create a decisiontreeregressor/classifier\n",
    "dtree = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the tree regressor/classifier\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "# Calculating the root mean squared error (RMSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WITH BIC FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 45.78660673398\n"
     ]
    }
   ],
   "source": [
    "# create a decisiontreeregressor/classifier\n",
    "dtree = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the tree regressor/classifier\n",
    "dtree.fit(X_train[:, best_features], y_train)\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred = dtree.predict(X_test[:, best_features])\n",
    "\n",
    "# Calculating the root mean squared error (RMSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WITH LASSO FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 45.78660673398\n"
     ]
    }
   ],
   "source": [
    "# create a decisiontreeregressor/classifier\n",
    "dtree = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the tree regressor/classifier\n",
    "dtree.fit(X_train[:, selected_features], y_train)\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred = dtree.predict(X_test[:, selected_features])\n",
    "\n",
    "# Calculating the root mean squared error (RMSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ensemble of Elastic Net Predictions + AdaBoost Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 0.9111627561154896\n",
      "\n",
      "For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n",
      "Optimal l1_ratio: 1.0\n",
      "Root MSE from OLS with elastic net regression and cross validation to find optimal lambda and model parameters:\n",
      "RMSE: 26.094967036910088\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import warnings\n",
    "\n",
    "# Setting a range of alphas to test\n",
    "alphas = np.logspace(-4, 0, 100)\n",
    "\n",
    "# Setting a range of l1_ratios\n",
    "l1_ratios = np.logspace(-10, 0, 100)\n",
    "\n",
    "with warnings.catch_warnings(): # done to disable all the convergence warnings from elastic net\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    # Fitting the Elastic Net model on the training data\n",
    "    model = ElasticNetCV(cv=3, l1_ratio = l1_ratios, alphas=alphas, fit_intercept=False).fit(X_train, y_train)\n",
    "\n",
    "# Printing the optimal alpha\n",
    "print(f'Optimal alpha: {model.alpha_}\\n')\n",
    "print('For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.')\n",
    "print(f'Optimal l1_ratio: {model.l1_ratio_}')\n",
    "\n",
    "# Using the optimal lambda from the ElasticNetCV model to predict the target values on the test data\n",
    "y_hat_elastic = model.predict(X_test)\n",
    "\n",
    "# Calculating the RMSE of the ElasticNetCV model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_hat_elastic))\n",
    "\n",
    "# Printing the RMSE\n",
    "print('Root MSE from OLS with elastic net regression and cross validation to find optimal lambda and model parameters:')\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n",
      "Test RMSE for AdaBoost: 55.8461\n"
     ]
    }
   ],
   "source": [
    "# Define parameter distributions\n",
    "param_dist = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'estimator__max_depth': [1, 3, 5, 10, 15],\n",
    "    'learning_rate': np.logspace(-2, 1, 50),\n",
    "}\n",
    "\n",
    "# Create AdaBoostRegressor with DecisionTreeRegressor as base estimator\n",
    "boost = AdaBoostRegressor(estimator=DecisionTreeRegressor())\n",
    "\n",
    "# Use RandomizedSearchCV for efficiency\n",
    "boost_search = RandomizedSearchCV(\n",
    "    boost,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=300,  # Adjust based on computational resources\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "boost_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "best_params = boost_search.best_params_\n",
    "best_score = -boost_search.best_score_  # Converting back to positive MSE\n",
    "\n",
    "# Use the best model to make predictions\n",
    "y_hat = boost_search.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_hat))\n",
    "print(f'Test RMSE for AdaBoost: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root MSE from weighted average of ElasticNetCV and AdaBoost models:\n",
      "RMSE: 39.37167178503088\n"
     ]
    }
   ],
   "source": [
    "# Performing an average of the predictions from the ElasticNetCV and AdaBoost models\n",
    "y_hat_weighted = (y_hat_elastic + y_hat_boost) / 2\n",
    "\n",
    "# Calculating the RMSE of the weighted average model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_hat_weighted))\n",
    "\n",
    "# Printing the RMSE\n",
    "print('Root MSE from weighted average of ElasticNetCV and AdaBoost models:')\n",
    "print(f'RMSE: {rmse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
