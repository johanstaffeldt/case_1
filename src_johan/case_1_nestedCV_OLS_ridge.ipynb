{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 - Ordinary Least Squares (OLS), Ridge Regression, and Elastic-Net Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Importing Libraries**\n",
    "\n",
    "2. **Loading Data**\n",
    "\n",
    "3. **Ordinary Least Squares (OLS)**\n",
    "\n",
    "4. **Ridge (L2) Regression**\n",
    "\n",
    "5. **Lasso (L1) Regression**\n",
    "\n",
    "6. **Elastic Net Regression**\n",
    "- Elastic Net Regression because we have many variables that we do not know. Elastic Net combines the strengths of Lasso regression (L1) and Ridge regression. Lasso regression can shrink parameters to 0 which is useful for large dataset where some parameters might be useless. Ridge regression tends to perform better when parameters are not useless. Therefore, elastic net is useful in this case as we do not know our parameters.\n",
    "- Find optimal model parameters, lamdba_1 and lambda_2 by using 5-fold cross validation.\n",
    "- Get root mean squared error (RMSE) by applying the model with the optimal model parameters on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default\n",
    "\n",
    "import scipy . linalg as lng\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data into numpy arrays\n",
    "X = np.loadtxt('../data/case1Data_X.csv', delimiter=',')\n",
    "y = np.loadtxt('../data/case1Data_y.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a range of alphas and l1_ratios to test\n",
    "alphas = np.logspace(-4, 1, 100) # Testing a range from very weak to strong regularization\n",
    "l1_ratios = np.concatenate(([0], np.logspace(-10, 0, 100))) # Testing a range from L2 (0) to L1 (1) regularization\n",
    "\n",
    "# Outer 5-fold cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rmse_scores = []\n",
    "alpha_scores = []\n",
    "l1_ratio_scores = []\n",
    "\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Inner loop for hyperparameter tuning (5-fold CV)\n",
    "    with warnings.catch_warnings():  # Suppress convergence warnings\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        model = ElasticNetCV(cv=5, l1_ratio=l1_ratios, alphas=alphas, fit_intercept=False).fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on the outer test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # Save the optimal alpha\n",
    "    alpha_scores.append(model.alpha_)\n",
    "\n",
    "    # Save the optimal l1_ratio\n",
    "    l1_ratio_scores.append(model.l1_ratio_)\n",
    "\n",
    "    # Saving the optimal model\n",
    "    if rmse == min(rmse_scores):\n",
    "        best_model = model\n",
    "\n",
    "    # Print the results of the inner loop\n",
    "    print(f'Fold RMSE: {rmse:.4f}')\n",
    "    print(f'Optimal alpha: {model.alpha_}')\n",
    "    print(f'Optimal l1_ratio: {model.l1_ratio_}\\n')\n",
    "\n",
    "# Final performance\n",
    "print(f'Average RMSE across outer folds: {np.mean(rmse_scores):.4f}')\n",
    "print(f'Average alpha: {np.mean(alpha_scores)}')\n",
    "print(f'Average l1_ratio: {np.mean(l1_ratio_scores)}')\n",
    "# Standard deviation of the RMSE tells us how much the RMSE varies between the folds (i.e., how stable the model is)\n",
    "print(f'Standard deviation of RMSE: {np.std(rmse_scores):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ordinary Least Squares (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold RMSE: 51.7609\n",
      "Fold RMSE: 41.5011\n",
      "Fold RMSE: 36.1615\n",
      "Fold RMSE: 58.5193\n",
      "Fold RMSE: 45.9665\n",
      "Average RMSE across outer folds: 46.7819\n",
      "Standard deviation of RMSE: 7.7962\n"
     ]
    }
   ],
   "source": [
    "# Function to solve the OLS\n",
    "def ols_solver(X, y):\n",
    "    betas, res, rnk, s = lng.lstsq(X, y)\n",
    "    return betas, res, rnk, s\n",
    "\n",
    "# Outer 5-fold cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rmse_scores = []\n",
    "\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Inner loop for hyperparameter tuning (5-fold CV)\n",
    "    with warnings.catch_warnings():  # Suppress convergence warnings\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        rmse_scores_inner = []\n",
    "        for train_idx_inner, test_idx_inner in inner_cv.split(X_train, y_train):\n",
    "            X_train_inner, X_test_inner = X_train[train_idx_inner], X_train[test_idx_inner]\n",
    "            y_train_inner, y_test_inner = y_train[train_idx_inner], y_train[test_idx_inner]\n",
    "\n",
    "            betas, _, _, _ = ols_solver(X_train_inner, y_train_inner)\n",
    "\n",
    "            # Evaluate on the inner test set\n",
    "            y_pred = X_test_inner @ betas\n",
    "\n",
    "            # Calculate the RMSE\n",
    "            rmse_inner = np.sqrt(mean_squared_error(y_test_inner, y_pred))\n",
    "            rmse_scores_inner.append(rmse_inner)\n",
    "\n",
    "            # Saving the optimal model\n",
    "            if rmse_inner == min(rmse_scores_inner):\n",
    "                best_betas_inner = betas\n",
    "\n",
    "    # Evaluate on the outer test set\n",
    "    y_pred = X_test @ best_betas_inner\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # Saving the optimal model\n",
    "    if rmse == min(rmse_scores):\n",
    "        best_betas = best_betas_inner\n",
    "\n",
    "    # Print the results of the inner loop\n",
    "    print(f'Fold RMSE: {rmse:.4f}')\n",
    "\n",
    "# Final performance\n",
    "print(f'Average RMSE across outer folds: {np.mean(rmse_scores):.4f}')\n",
    "# Standard deviation of the RMSE tells us how much the RMSE varies between the folds (i.e., how stable the model is)\n",
    "print(f'Standard deviation of RMSE: {np.std(rmse_scores):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ridge (L2) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold RMSE: 33.5101\n",
      "Fold RMSE: 29.7820\n",
      "Fold RMSE: 28.9537\n",
      "Fold RMSE: 34.3106\n",
      "Fold RMSE: 29.9738\n",
      "Average RMSE across outer folds: 31.3060\n",
      "Standard deviation of RMSE: 2.1687\n"
     ]
    }
   ],
   "source": [
    "# Setting k for the number of lambdas to test\n",
    "k = 1000\n",
    "\n",
    "# Creating a list of lambdas to test\n",
    "lambdas = np.logspace(-1, 4, k)\n",
    "\n",
    "# Outer 5-fold cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rmse_scores = []\n",
    "\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Inner loop for hyperparameter tuning (5-fold CV)\n",
    "    with warnings.catch_warnings():  # Suppress convergence warnings\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        model = RidgeCV(alphas=lambdas, store_cv_values=False, cv=5).fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on the outer test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # Saving the optimal model\n",
    "    if rmse == min(rmse_scores):\n",
    "        best_model = model\n",
    "\n",
    "    # Print the results of the inner loop\n",
    "    print(f'Fold RMSE: {rmse:.4f}')\n",
    "\n",
    "# Final performance\n",
    "print(f'Average RMSE across outer folds: {np.mean(rmse_scores):.4f}')\n",
    "# Standard deviation of the RMSE tells us how much the RMSE varies between the folds (i.e., how stable the model is)\n",
    "print(f'Standard deviation of RMSE: {np.std(rmse_scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lasso (L1) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold RMSE: 28.5733\n",
      "Fold RMSE: 21.1610\n",
      "Fold RMSE: 24.4641\n",
      "Fold RMSE: 24.5089\n",
      "Fold RMSE: 28.4905\n",
      "Average RMSE across outer folds: 25.4396\n",
      "Standard deviation of RMSE: 2.8019\n"
     ]
    }
   ],
   "source": [
    "# Outer 5-fold cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rmse_scores = []\n",
    "models = []\n",
    "\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Inner loop for hyperparameter tuning (5-fold CV)\n",
    "    with warnings.catch_warnings():  # Suppress convergence warnings\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        model = LassoCV(cv=5, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on the outer test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    # Saving the model\n",
    "    models.append(model)\n",
    "\n",
    "    # Saving the optimal model\n",
    "    if rmse == min(rmse_scores):\n",
    "        best_model = model\n",
    "\n",
    "    # Print the results of the inner loop\n",
    "    print(f'Fold RMSE: {rmse:.4f}')\n",
    "\n",
    "# Final performance\n",
    "print(f'Average RMSE across outer folds: {np.mean(rmse_scores):.4f}')\n",
    "# Standard deviation of the RMSE tells us how much the RMSE varies between the folds (i.e., how stable the model is)\n",
    "print(f'Standard deviation of RMSE: {np.std(rmse_scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predicting $\\hat{y}$ in the new data set (case1Data_Xnew.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading case1Data_Xnew_wrangled.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new:  (1000, 116)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data into numpy arrays\n",
    "X_new = np.loadtxt('../data/case1Data_Xnew_wrangled.csv', delimiter=',')\n",
    "print(\"X_new: \", X_new.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting and saving predictions in a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# Predicting y_hat for the data in case1Data_Xnew.csv using the best model\n",
    "y_hat_new = best_model.predict(X_new)\n",
    "\n",
    "# Printing the shape of the new data\n",
    "print(y_hat_new.shape)\n",
    "\n",
    "# Saving the predictions to a csv file\n",
    "np.savetxt('../results/sample_predictions_s183220_s225001.csv', y_hat_new, delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting y_hat for the data in case1Data_Xnew.csv using an ensemble of the models\n",
    "y_hat_new_ensemble = np.zeros(X_new.shape[0])\n",
    "for model in models:\n",
    "    y_hat_new_ensemble += model.predict(X_new)\n",
    "y_hat_new_ensemble /= len(models)\n",
    "\n",
    "# Saving the predictions to a csv file\n",
    "np.savetxt('../results/sample_predictions_s183220_s225001.csv', y_hat_new_ensemble, delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Estimated RMSE to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected RMSE: 25.73\n"
     ]
    }
   ],
   "source": [
    "# We will use the average RMSE across the outer folds as the expected RMSE\n",
    "# Saving the value as float with 2 decimals precision\n",
    "print(f'Expected RMSE: {np.mean(rmse_scores):.2f}')\n",
    "np.savetxt('../results/sample_estimatedRMSE_s183220_s225001.csv', [np.mean(rmse_scores)], fmt='%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
