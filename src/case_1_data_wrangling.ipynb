{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 - Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. **Importing Libraries**\n",
    "\n",
    "2. **Loading Data**\n",
    "\n",
    "3. **Wrangling case1Data.csv**\n",
    "\n",
    "4. **Wrangling case1Data_Xnew.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Imputers\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Standardization scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case1Data.csv:  (100, 101)\n",
      "case1Data_Xnew.csv:  (1000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data from case1Data.csv into a numpy array\n",
    "data = np.loadtxt('../data/case1Data.csv', delimiter=',', skiprows=1)\n",
    "print(\"case1Data.csv: \", data.shape)\n",
    "\n",
    "# Loading the new data into a numpy array\n",
    "X_new = pd.DataFrame(np.loadtxt('../data/case1Data_Xnew.csv', delimiter=',', skiprows=1))\n",
    "print(\"case1Data_Xnew.csv: \", X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrangling case1Data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (100, 100)\n",
      "y:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into features (X) and target (y)\n",
    "X = data[:, 1:] # All columns except the first one\n",
    "y = data[:, 0] # First column\n",
    "print(\"X: \", X.shape)\n",
    "print(\"y: \", y.shape)\n",
    "\n",
    "# Saving the data in a csv file\n",
    "np.savetxt('../data/case1Data_X.csv', X, delimiter=',')\n",
    "np.savetxt('../data/case1Data_y.csv', y, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wrangling case1Data_Xnew.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new:  (1000, 116)\n"
     ]
    }
   ],
   "source": [
    "# Using StandardScaler from scikit-learn to standardize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardizing the numerical features (all columns exept the last five)\n",
    "X_new.iloc[:, :-5] = scaler.fit_transform(X_new.iloc[:, :-5])\n",
    "\n",
    "# Using KNNImputer from scikit-learn to impute the missing values in the data (for continuous variables) with the mean of the k-nearest neighbors (k=5)\n",
    "continuous_imputer = KNNImputer(n_neighbors=5, missing_values=np.nan)\n",
    "X_new.iloc[:, :-5] = pd.DataFrame(continuous_imputer.fit_transform(X_new.iloc[:, :-5]))\n",
    "\n",
    "# Mode Imputation: Using SimpleImputer from scikit-learn to impute the missing values in the data (for categorical variables) with the most frequent value\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_new.iloc[:, -5:] = categorical_imputer.fit_transform(X_new.iloc[:, -5:])\n",
    "\n",
    "# One-hot encoding the categorical variables using get_dummies from pandas library (for the last five columns)\n",
    "X_new = pd.get_dummies(X_new, columns=X_new.columns[-5:])\n",
    "\n",
    "# Converting the data into numpy arrays\n",
    "X_new = np.asarray(X_new, dtype=np.float64)\n",
    "\n",
    "# Saving the preprocessed data to a csv file\n",
    "np.savetxt('../data/case1Data_Xnew_wrangled.csv', X_new, delimiter=',')\n",
    "print(\"X_new: \", X_new.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
